# #+TITLE: Classical Planning in Deep Discrete Latent Space: Neural Symbolic Architectures and Symbol Grounding
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg
#+LINK: svg file:img/%s.svg
#+LINK: gif file:img/%s.gif
#+LINK: spng file:img/static/%s.png
#+LINK: sjpg file:img/static/%s.jpg
#+html_head: <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:500,900">
#+html_head_extra:
#+LINK: ssvg file:img/static/%s.svg
#+LINK: sgif file:img/static/%s.gif

#+begin_outline-text-1
#+begin_center

# [[png:latplanlogo]]

#+begin_larger
*Learning Neural-Symbolic Descriptive Planning Models*

*via Cube-Space Priors*

　

*The Voyage Home*

*(to STRIPS)*
#+end_larger

　

#+begin_larger
Masataro Asai (IBM Research), Christian Muise (U.Queens)
#+end_larger

　

　

[[png:MIT-IBM]]

# [[spng:ibm-research]]

#+end_center

#+end_outline-text-1

* Classical Planners *solve 8-puzzles << .1sec.*

#+begin_container-fluid
#+begin_container-row
#+begin_span2

#+end_span2
#+begin_span4
[[png:8puzzle-standard]]
#+end_span4
#+begin_span4
[[png:8puzzle-standard-goal]]
#+end_span4
#+begin_span2
[[sgif:8puzzle]]
#+end_span2
#+end_container-row
#+end_container-fluid

/but only when we have a PDDL model./

#+begin_container-fluid
#+begin_row-fluid
#+begin_span12
#+begin_src lisp
(:action slide-up ...
 :precondition (and (empty ?x ?y-old) ...)
 :effects (and (not (empty ?x ?y-old))...))
#+end_src
#+end_span12
#+end_row-fluid
#+end_container-fluid

* They cannot solve an */Image-based/* 8-puzzle

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
[[sjpg:puzzle]]
#+end_span8
#+begin_span4
+ 
   #+begin_center
   *BECAUSE*

   *WE*

   *DO*

   *NOT*

   *HAVE*

   *A*

   #+begin_larger
   */PDDL/*

   */MODEL/!*
   #+end_larger
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

+ *Latplan (AAAI 2018)* addresses this problem:

  */No Prior Explicit Knowledge/* (i.e. No Human Annotation)

  　 */No labels/symbols given/* : e.g. "9 tiles", "moving"

* Knowledge-Acquisition Bottleneck:                                :noexport:

# * We must *automate 2 processes*:

# * Knowledge-Acquisition Bottleneck:
# 
# #+begin_quote
# The *cost of human* involved for converting *real-world problems* into the inputs for
# domain-independent *symbolic* systems
# #+end_quote



#+begin_container-fluid
#+begin_row-fluid
#+begin_span4

#+begin_alignright
*Visual observations*
#+end_alignright

[[png:overview/1]]

#+end_span4
#+begin_span1
　

　

　

→

　
#+end_span1
#+begin_span7
#+begin_center
*PDDL Model*
#+end_center
#+begin_src lisp
(:action slide-up
 :precondition
 (and (empty ?x ?y-old) ...)
 :effects
 (and (not (empty ?x ?y-old))...))
#+end_src

#+end_span7
#+end_row-fluid

#+begin_row-fluid
#+begin_span12
+ We must *automate 2 processes*:
#+end_span12
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *1. Symbol Grounding:*

  #+begin_center
  #+begin_larger
  */Symbols/ = words in PDDL*
  #+end_larger
  #+end_center
  
  #+begin_smaller
  | Types        | Examples                     |
  |--------------+------------------------------|
  | Objects      | *panel_7*, *x_0*, *y_0* ...  |
  | Predicates   | (*empty* ?x ?y)              |
  | Propositions | *p_28* = (empty x_0 y_0)     |
  | Actions      | (*slide-up* panel_7 x_0 y_1) |
  #+end_smaller
#+end_span6
#+begin_span6
+ *2. Action Model Acquisition:*
  
  #+begin_center
  #+begin_larger
  */Describe/ the*

  */transition rules/*

  *with symbols.*
  #+end_larger
  #+end_center
  
  　

  #+begin_center
  *When* /Empty(x, y_{old}) ∧ .../ ;

  *Then* /¬Empty(x,y_{old}) ∧/ ...
  #+end_center

#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_note
# The knowledge acquisition bottleneck: time for reassessment? : Cullen, J and Bryman, A Expert Syst. Vol 5 No 3 (August 1988) pp 216-225
# #+end_note

* Latplan (Asai,Fukunaga 2018): Solving an Imaged-Based 8-puzzle @@html:<br>@@ */without any Prior Explicit Knowledge/* @@html:<br>@@ (i.e. No Human Annotation) :noexport:

#+begin_center
*/System is not given any labels/symbols/* : e.g. "9 tiles", "moving"
#+end_center

[[sjpg:puzzle]]

** Latplan system: Solving */ANY/* Imaged-Based task               :noexport:

#+begin_larger
→ */Image-based Domain-independent Classical Planner/*
#+end_larger

　

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_center
*Tower of Hanoi*

[[sjpg:hanoi]]
#+end_center
#+end_span6
#+begin_span4
#+begin_center
*Lights-Out*

[[sjpg:lightsout]]
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Input1: Training Input -- Image Pairs                           :noexport:

#+begin_center
*an unknown high-level action* between
#+end_center

[[png:overview/1]]

#+begin_center
(The tile 6 has moved)
#+end_center

** Input1: Training Input -- Image Pairs                           :noexport:

#+begin_right
[[png:overview/2]]
#+end_right

*Randomly sampled transitions as noisy image pairs*

+ No *state descriptions* (unlike AlphaGo)
+ No *expert traces* (unlike initial AlphaGo)
+ No *rewards* (unlike DRL in general)
  
  → We don't learn the *policies tied to the domain (less general)*
+ No *access to the simulator* (unlike DQN+Atari)
  
  → cannot ask for more examples
+ 
  #+begin_larger
  No */action symbols/*

  　　(e.g. ↑↓←→Fire in Atari, grids in Go)

  # → unlike *any previous work to our knowledge*,
  # 
  # 　 Image pairs are */unlabelled/ ground actions*
  #+end_larger

# + *ALL* previous Acion Modelling systems require */symbolic/near-symbolic state/action inputs/*.
# 
#   (Konidaris et al. 2014, 2015), ARMS (Yang et al. 07), LOCM (Cresswell et
#   al. 09), (Argall et al. 09), (Mourao et al. 12), Framer (Lindsay et al. 17)

#+begin_note
DQN (Mnih et al. '15), AlphaGo (Silver et al. '16)
#+end_note

** Input2: Planning Input = Initial/Goal States                    :noexport:

[[png:overview/input2]]

* Latplan system I/O                                               :noexport:

[[png:overview/3-2]]

** Latplan system I/O

[[png:overview/3-1]]

** Latplan system I/O

[[png:overview/3]]

** Output: Visualized Plan                                         :noexport:

#+HTML: <embed src="img/overview/3-hanoi.svg" type="image/svg+xml"  />

* Latplan architechture                                            :noexport:

  [[png:overview/planning1]]

* Step 1: Propositional Symbol Grounding

[[png:overview/planning2]]

** Step 2: 

[[png:overview/planning3]]

** Step 3: 

[[png:overview/planning4]]

** Step 4: 

[[png:overview/planning5]]

** Step 5: 

[[png:overview/planning6]]

#+begin_center
#+begin_larger
#+end_larger
#+end_center

* AutoEncoder (AE): Unsupervised Learning for NN

# Auto = "self" --- Autoencoding = "encoding itself"

# + $z\;$ has a smaller dimension
# + i.e. Compression: $X \leftrightarrow Z$

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
　

Tries to perform *lossless compression*

Target Function: Identity $x=f(x)$

　Encoder: map $x\;$ to a *latent vector* $z$
  
　Decoder: map $z\;$ back to the input $x$
  
　Training: Minimize $|x - f(x)|\;$

　(*reconstruction loss*)
#+end_span7
#+begin_span5
[[png:deeplearning/autoenc]]
#+end_span5
#+end_row-fluid
#+end_container-fluid

#+begin_center
#+begin_larger
+ */Latent vector Z is real-valued/*
  
  →  */INCOMPATIBLE with / Useless for/*

  *the _propositional logic reasoning_ in Classical Planning.*
#+end_larger
#+end_center

* Discrete Variational AutoEncoders (VAEs)

There are *many* ways to learn discrete vectors these day, check out!!!

#+begin_quote
Straight-Through Estimator (Bengio et al, 2013)

　  ← Used by (Koul&Fern et al, ICLR18)

*Gumbel-Softmax/Binary-Concrete*

DVAE++ / DVAE# / VQ-VAE / VQ-VAE2
#+end_quote

#+begin_note
(Jang et al / Maddison et al, ICLR17), (Vahdat et al, ICML18 / Neurips18), (van den Oord et al, Neurips17), (Rasavi et al, Neurips19)
#+end_note

# Enforces $Z \sim \textbf{Categorical}$:
# 
# 　　　→ $z\;$ converges to a 1-hot vector e.g.  $\langle 0,0,1,0 \rangle$ .
# 
# \begin{align*}
#   x \in \mathbb{R}^N, \quad
#   z = \text{GumbelSoftmax}(x) &= \text{Softmax}( (x + \epsilon) / \tau )                         &\in [0,1]^N      \\
#   \text{Softmax}(x)       &= \frac{(e^{x_1}, e^{x_2},\ldots e^{x_N})}{\sum_j e^{x_j}} &\in [0,1]^N      \\
#   \epsilon = \text{Gumbel}(0,1)  &= - \log (- \log \text{Uniform}(0,1))                      &\in \mathbb{R}^N \\
# (\tau \downarrow 0) \quad \text{GumbelSoftmax}(x) &\rightarrow \arg \max(x)               &\in \{0,1\}^N \\
# D_{\text{KL}} &= \sum_i \text{Softmax}(x)_i\cdot \log\text{Softmax}(x)_i
# \end{align*}

# Example: Represent an MNIST image with 30 variables of 8 categories.
# 
# #+begin_center
#  #+begin_html
#  <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="649px" height="206px" version="1.1" content="&lt;mxfile userAgent=&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&quot; version=&quot;6.0.1.2&quot; editor=&quot;www.draw.io&quot; type=&quot;google&quot;&gt;&lt;diagram name=&quot;Page-1&quot;&gt;3ZhLc5swEMc/Dcd0kAQCX+O67SGd6TTTaXqU0fJoZcsjy69++oogDBjs0MZ2TONDpL9ey29Xi5BDxrPtR8UW6WfJQTjY5VuHvHcwxshF5l+u7AqF+rgQEpXxQkKV8Jj9Biu6Vl1lHJaNjlpKobNFU4zkfA6RbmhMKblpdoulaK66YAm0hMeIibb6PeM6LdTQdyv9E2RJWq6MXNsyY2VnKyxTxuWmJpGJQ8ZKSl2UZtsxiBxeyaUY9+FI694wBXPdZ0AQh1EYx4xP46kXRHBnZ1gzsbIPaw3Vu/LpN2mm4XHBory+MR52yH2qZ8LUkCnGmRBjKaR67k0AcR8Coy+1kr+g1jKiAWHUtCi5mnPgdrw1AJSG7dGnQntWJshAzkCrneliB5CAFENsfCHPL+qbylt7n6Q1T+HAisxGSLKfu4JoCpZjT6a4xRT71MFUmFXvebY2xSQvfoWHb6VsFqm1tHygUjmbroyR9y944ww08QFN7LktmmEHzPASLEmLpY/wcFiiW2LptVAAN3nOVqXSqUzknIlJpdb2qtuEA9tMP9XKP/Iu7/y8NjeGPtkRz5Wq7SdovbMJnq20NFK17oOUiwb63LzT4M3TyJWK4HT0aKYS0Kd2a9uBCgTT2bq5/mvc4eEIU4xG4PsesCDqCO1z+ufKpBHugZq8FeorRf6VmfdBfiQ9XR45HXbi9kf4zRJ3i2Uw7AOFd0ssw//9JXgseupponu3Ht8MwYEDL+KZ0cDehyeBBqd3BB31B2pn+SIzY0D/KYqYsKMO3LK36N88hc7+1cgZhHHU9dVIoxCm8SW+GukINQm67bSEaEde2otnDX901nAfRGaiPTYSutYJ5uiRdthRfldeot1IlPfI8i9Abcb8AeLYz3+diJ//Xgm1hIg6IHpdEL1LQCxvEwcK0Q4I3IP8+8ZQX/9SuwGoJGxud+STFtSQtpli3/trpqZa3VwX54rq/p9M/gA=&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g transform="translate(0.5,0.5)"><rect x="288" y="0.75" width="75" height="202.5" rx="11.25" ry="11.25" fill="#e1d5e7" stroke="#9673a6" pointer-events="none"/><path d="M 243 72 L 273 102 L 243 132 L 213 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(231.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">256<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 168 72 L 198 102 L 168 132 L 138 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(156.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">512<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 198 102 L 208.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 212.16 102 L 206.91 104.63 L 208.22 102 L 206.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 120.75 102 L 135.75 102 L 123 102 L 133.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 137.16 102 L 131.91 104.63 L 133.22 102 L 131.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 273 102 L 288 102 L 273 102 L 283.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 287.16 102 L 281.91 104.63 L 283.22 102 L 281.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 482.25 72 L 512.25 102 L 482.25 132 L 452.25 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(470.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">512<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 407.25 72 L 437.25 102 L 407.25 132 L 377.25 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(395.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">256<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 437.25 102 L 447.47 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 451.41 102 L 446.16 104.63 L 447.47 102 L 446.16 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 360 102 L 375 102 L 362.25 102 L 372.47 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 376.41 102 L 371.16 104.63 L 372.47 102 L 371.16 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="526.5" y="42" width="120" height="120" rx="18" ry="18" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><path d="M 512.25 102 L 521.72 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 525.66 102 L 520.41 104.63 L 521.72 102 L 520.41 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="0.75" y="42" width="120" height="120" rx="18" ry="18" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/>
#  <image xlink:href="img/static/x0.gif" x="8.25" y="49.5" width="105" height="105" fill="#f5f5f5" stroke="#666666" pointer-events="none"/>
#  <image xlink:href="img/static/x1.gif" x="534" y="49.5" width="105" height="105" fill="#f5f5f5" stroke="#666666" pointer-events="none"/>
#  <image xlink:href="img/static/y.gif" x="293.25" y="6.75" width="64.5" height="190.5" fill="#f5f5f5" stroke="#666666" pointer-events="none"/></g></svg>
#  #+end_html
# #+end_center

#+begin_center
#+begin_larger
+ *Categorical variables are /directly/ compatible*

  *with symbolic systems.*

  *2 categories → /propositional/ variables (true/false).*

#+end_larger
#+end_center

** State Autoencoder (_/after training/_)

[[png:sae/state-ae]]

* Action Model Acquisition (AMA2) Overview

[[png:ama/overview0]]

** Action Model Acquisition (AMA2) Overview

 [[png:ama/overview1]]

** Action Model Acquisition (AMA2) Overview

  [[png:ama2/overview3-minus-ad]]

* Action Auto Encoder (AAE)

 [[png:aae2/aae1]]
** Action Auto Encoder (AAE)

 [[png:aae2/aae2]]
** Action Auto Encoder (AAE)

 [[png:aae2/aae3]]
** Action Auto Encoder (AAE)

 [[png:aae2/aae4]]
** Action Auto Encoder (AAE)

 [[png:aae2/aae5]]
** Action Auto Encoder (AAE)

 [[png:aae2/aae6]]

** Not all actions are applicable                                  :noexport:

 [[png:ama2/overview1-1]]

** Not all actions are applicable                                  :noexport:

 [[png:ama2/overview1-2]]

** Not all actions are applicable                                  :noexport:

 [[png:ama2/overview1-3]]

** Not all actions are applicable                                  :noexport:

 [[png:ama2/overview1-4]]

** Not all actions are applicable                                  :noexport:

 [[png:ama2/overview1-5]]


* Do the NN-generated propositions */make sense?/*                 :noexport:

*AMA_1* : an */oracular/* Action Model Acquisition (AMA) method

[[png:ama1/slide5]]

** Do the NN-generated propositions */make sense?/*

 *AMA_1* : an */oracular/* Action Model Acquisition (AMA) method

 [[png:ama1/slide4]]

** Do the NN-generated propositions */make sense?/*

 *AMA_1* : an */oracular/* Action Model Acquisition (AMA) method

 [[png:ama1/slide3]]

** Do the NN-generated propositions */make sense?/*

 *AMA_1* : an */oracular/* Action Model Acquisition (AMA) method

 [[png:ama1/slide2]]

** Do the NN-generated propositions */make sense?/*

 *AMA_1* : an */oracular/* Action Model Acquisition (AMA) method

 [[png:ama1/slide1]]

** Do the NN-generated propositions */make sense?/*

 *AMA_1* : an */oracular/* Action Model Acquisition (AMA) method

 [[png:ama1/slide0]]

** Do the NN-generated propositions */make sense?/*

 *AMA_1* : an */oracular/* Action Model Acquisition (AMA) method

 [[png:ama1/slide]]

* Do the NN-generated propositions */make sense?/*                 :noexport:

#   @@html:<br>@@ Note: They are learned unsupervised!

*AMA_1* : an */oracular/* Action Model Acquisition (AMA) method

　　　　　 *w/o generalization*.

+ 1. *Train the State AutoEncoder (SAE)* with the training images
+ 2. *Encode all images* in the environment
+ 3. For each transition, *convert the latent vector pairs to a PDDL action:*

  #+begin_src lisp
   0011 → 0101  ;; encoded bit vectors of a transition
  
  　　　↓       ;; one action per transition
  (:action       action-0011-0101

   :precondition (and (b0-false) (b1-false) (b2-true) (b3-true))

                 ;; effect = state difference
   :effect       (and (not (b1-false)) (b1-true)
                      (not (b2-true))  (b2-false)))
  #+end_src
+ 4. Solve the PDDL with Fast Downward (Helmert 08) with A^* algorithm

** Step 3: Solve the Symbolic Planning Problem                     :noexport:

  [[png:overview/planning4]]

** Step 4: Executing the Symbolic Plan                             :noexport:

  [[png:overview/planning5]]

** Step 5: Obtaining the Visualization                             :noexport:

  [[png:overview/planning6]]

** AMA_1 Experiments                                               :noexport:

 #+begin_quote
 #+begin_center
 Show that the State AE (neural network) produce */sound propositions/*
 #+end_center
 #+end_quote

 State AE: *trained with a subset of images*

 AMA_1 : *oracular method* which uses *the entire transitions*

 Planner: a State-of-the-Art, Fast Downward (Helmert, 08)

 + $A^*$ *(optimal search)* : *It must find an optimal solution*

 + Runtime: ~3sec (instances are too small for symbolic systems)

* 8-puzzle with MNIST tiles (MNIST 8-puzzle)                       :noexport:

*If the representation is correct, it _guarantees the optimal solution_*.

[[png:results/mnist-plan]]

#+begin_larger
#+begin_xlarge
#+begin_alignright
 → *31 step optimal plan*
#+end_alignright
#+end_xlarge

#+begin_center
 → *the representation is correct*
#+end_center
#+end_larger

* AMA_1 (Oracular) → */AMA_2 (Learning-based)/*                   :noexport:

[[png:ama/overview0]]

** Action Auto Encoder

 [[png:ama/overview1]]

** Action Auto Encoder

 [[png:aae2/aae1]]
** Action Auto Encoder

 [[png:aae2/aae2]]
** Action Auto Encoder

 [[png:aae2/aae3]]
** Action Auto Encoder

 [[png:aae2/aae4]]
** Action Auto Encoder

 [[png:aae2/aae5]]
** Action Auto Encoder

 [[png:aae2/aae6]]

** Not all actions are applicable

 [[png:ama2/overview1-1]]

** Not all actions are applicable

 [[png:ama2/overview1-1]]

** Not all actions are applicable

 [[png:ama2/overview1-2]]

** Not all actions are applicable

 [[png:ama2/overview1-3]]

** AMA_2 Overview

  [[png:ama2/overview3]]

* Overview                                                         :noexport:

#+begin_center
#+begin_xlarge
*Background*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*
 
*/Action AutoEncoder (AAE)/*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

** Implementing AAE

[[png:aae/aae-0]]

#+begin_center
*/We skip a lot of interesting motivations here!!/*

motivation: no action label, unsupervised learning, standard predictor network not applicable
#+end_center

** Implementing AAE                            :noexport:

[[png:aae/aae-1]]

#+begin_center
*/We skip a lot of interesting motivations here!!/*

motivation: no action label, unsupervised learning, std network not applicable
#+end_center

** Implementing AAE                            :noexport:

[[png:aae/aae-2]]

#+begin_center
*/We skip a lot of interesting motivations here!!/*
#+end_center

** Implementing AAE

[[png:aae/aae-3]]

#+begin_center
*/We skip a lot of interesting motivations here!!/*

motivation: no action label, unsupervised learning, standard predictor network not applicable
#+end_center

* Overview                                                         :noexport:

#+begin_center
#+begin_xlarge
*Background*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*/Action Discriminator (AD)/*
#+end_xlarge
#+end_center

** Action Discriminator learns preconditions                       :noexport:

  [[png:ama/overview2]]

** Action Discriminator learns Preconditions = Binary Classifer

Trained by a */positive(valid)/* and */negative(invalid)/* dataset

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:aae/ad]]
#+end_span6
#+begin_span6
Positive = Training Input
#+begin_center
+ *Where are the negative (/invalid/) datasets?*
#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Negative datasets are */fundamentally/* unavailable in the real world

# + */Invalid transitions/*: *All transitions that are not valid.*
#   + We lack the definition; *Cannot be generated.*

*Invalid transitions /never happen./*

E.g. *Teleportation violates the laws of physics*. (at least in a macro scale)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span6
[[png:aae/teleportation]]
#+end_span6
#+begin_span2

#+end_span2
#+end_row-fluid
#+end_container-fluid

#+begin_alignright
+ 
  #+begin_larger
  *They cannot be /observed/ in the real world.*
  
  → */Negative datasets are not available/*
  #+end_larger

  without the help from human.
#+end_alignright

** Soluton: PU-Learning (Elkan & Noto, KDD 08')

Learning a *positive / negative classifier* from *positive / /unlabelled/ datasets*.

+ Unlabelled: *Successor candidates generated by the AAE*
  
  Some are valid, some are invalid

* Experiments                                                      :noexport:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:results/mandrill-intro-new]]
[[png:results/mandrill-plan-new]]
[[png:results/hanoi4]]
[[png:results/lightsout_new4x4]]
[[png:results/lightsout_twisted_new4x4]]
#+end_span6
#+begin_span6
[[png:results/noise-new]]
[[png:results/spider-plan-new]]
[[png:results/mnist-plan-new]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

* AMA_2 Experiments                                                :noexport:

Is it feasibile to do planning with AAE and AD?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
*Easy instances: Majority of instances are solved*

*Harder instances: Still many instances are solved*

#+begin_smaller
|   | /          |   < |     |   > |   < |    |   > |
|   | step       |   7 |   7 |   7 |  14 | 14 |  14 |
|   | noise      | std |   G | s/p | std |  G | s/p |
|---+------------+-----+-----+-----+-----+----+-----|
|   | MNIST      |  72 |  64 |  64 |   6 |  4 |   3 |
|   | Mandrill   | 100 | 100 | 100 |   9 | 14 |  14 |
|   | Spider     |  94 |  99 |  98 |  29 | 36 |  38 |
|   | LightsOut  | 100 |  99 | 100 |  59 | 60 |  51 |
|   | Twisted LO |  96 |  65 |  98 |  75 | 68 |  72 |
| / | Hanoi      |  37 |  44 |  39 |  15 | 18 |  17 |
#+end_smaller
#+end_span6
#+begin_span6
[[png:results/mandrill-intro-new]]
[[png:results/mandrill-plan-new]]
[[png:results/hanoi4]]
[[png:results/lightsout_new4x4]]
[[png:results/lightsout_twisted_new4x4]]
[[png:results/noise-new]]
[[png:results/spider-plan-new]]
[[png:results/mnist-plan-new]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

** AMA_2 Experiments                                               :noexport:

Is it feasibile to do planning with AAE and AD?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
*Easy instances: Majority of instances are solved*

*Harder instances: Some instances are solved*

#+begin_smaller
|   | /          |   < |     |   > |   < |    |   > |
|   | step       |   7 |   7 |   7 |  14 | 14 |  14 |
|   | noise      | std |   G | s/p | std |  G | s/p |
|---+------------+-----+-----+-----+-----+----+-----|
|   | MNIST      |  72 |  64 |  64 |   6 |  4 |   3 |
|   | Mandrill   | 100 | 100 | 100 |   9 | 14 |  14 |
|   | Spider     |  94 |  99 |  98 |  29 | 36 |  38 |
|   | LightsOut  | 100 |  99 | 100 |  59 | 60 |  51 |
|   | Twisted LO |  96 |  65 |  98 |  75 | 68 |  72 |
| / | Hanoi      |  37 |  44 |  39 |  15 | 18 |  17 |
#+end_smaller
#+end_span6
#+begin_span6
*Action Discriminators achieved reasonable accuracy*

the type-1 / type-2 error in %

|          | type1 | type2 |
|----------+-------+-------|
| MNIST    |  1.55 |  6.15 |
| Mandrill |  1.10 |  2.93 |
| Spider   |  1.22 |  4.97 |
| L. Out   |  0.03 |  1.64 |
| Twisted  |  0.02 |  1.82 |
| Hanoi    |  0.25 |  3.79 |
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Experimental setting                                            :noexport:

+ *100 instances* for each domain
  + self-avoiding random walks from the goal state
  + (benchmark A) 7-step, (benchmark B) 14-step
  + no / gaussian / salt-pepper noise
+ 180 sec. time limit
+ Domain-specific plan validators.

# The failures are due to timeouts
# (the successor function requires many calls to the feedforward neural nets,
#  resulting in a very slow node generation).

# We next examine the accuracy of the AD and SD (\reftbl{tab:aae-results}).
# We measured the type-1/2 errors for the valid and invalid transitions (for AD) and states (SD).
# Low errors show that our networks successfully learned the action models.

** AMA_2 Experiments 2                                             :noexport:

How accurate are Action Discriminators and State Discriminators?


#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
Measure the type-1 / type-2 error in %

#+begin_smaller
|          |    SD |    SD |   |    AD |    AD |   AD |   AD |
|          | type1 | type2 |   | type1 | type2 | 2/SD |  2/V |
|----------+-------+-------+---+-------+-------+------+------|
| MNIST    |  0.09 | <0.01 |   |  1.55 |  14.9 | 6.15 | 6.20 |
| Mandrill | <0.01 | <0.01 |   |  1.10 |  16.6 | 2.93 | 2.94 |
| Spider   | <0.01 | <0.01 |   |  1.22 |  17.7 | 4.97 | 4.91 |
| L. Out   | <0.01 |   N/A |   |  0.03 |  1.64 | 1.64 | 1.64 |
| Twisted  | <0.01 |   N/A |   |  0.02 |  1.82 | 1.82 | 1.82 |
| Hanoi    |  0.03 | <0.01 |   |  0.25 |  3.50 | 3.79 | 4.07 |
#+end_smaller

#+end_span7
#+begin_span5
#+begin_smaller
+ (SD type-1) :: Generate all valid states and count the states misclassified as invalid.
+ (SD type-2) :: Generate reconstructable states, remove the valid states (w/ validator),
                 sample 30k states, and count the states misclassified as valid.
                 N/A means all reconstructable states were valid.
+ (AD type-1) :: Generate all valid transitions and count the number of misclassification.
+ (AD type-2) :: For 1000 randomly selected valid states, generate all successors,
                 remove the valid transitions (w/ validator), then count the transitions misclassified as valid.
+ (2/SD, 2/V) :: Same as Type-2, but ignore the transitions whose successors are
                 invalid according to SD or the validator.
#+end_smaller
#+end_span5
#+end_row-fluid
#+end_container-fluid

** AMA_2 Experiments 2                                             :noexport:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
How accurate are Action Discriminators?

Measure the type-1 / type-2 error in %

|          | type1 | type2 |
|----------+-------+-------|
| MNIST    |  1.55 |  6.15 |
| Mandrill |  1.10 |  2.93 |
| Spider   |  1.22 |  4.97 |
| L. Out   |  0.03 |  1.64 |
| Twisted  |  0.02 |  1.82 |
| Hanoi    |  0.25 |  3.79 |

#+end_span6
#+begin_span6
#+begin_smaller
+ (AD type-1) :: Generate all valid transitions and count the number of misclassification.
+ (AD type-2) :: For 1000 randomly selected valid states, generate all successors,
                 prune by SD,
                 remove the valid transitions (w/ validator), then count the transitions misclassified as valid.
# + (2/SD, 2/V) :: Same as Type-2, but ignore the transitions whose successors are
#                  invalid according to SD or the validator.
#+end_smaller
#+begin_larger
+ *Reasonably accurate.*
#+end_larger
#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_larger
# #+begin_alignright
# + *Reasonably accurate.*
# #+end_alignright
# #+end_larger

* Main Contribution: Scaling up Latplan @@html:<br>@@ with */SotA Classical Planning heuristics/*

*8-Puzzle* ($!9< 400k$ states) $\longrightarrow$ *15-Puzzle* ($!16\approx 10^{13}$ states)

+ Scalability requires *SotA heuristics* to tackle the problem
  + *domain-independent* : because our domains are *generated by NN*

* Heuristic functions                                              :noexport:

+ You know RL learns a Value function $V(s)$
  + *higher the value, better the state*
+ Classical planning have the same function *without learning*
  + called a *Heuristic Function* $h(s)$
  + = cost estimate to reach the goal states: *lower the better*

** The current most successful heuristic function : Delete Relaxation

 [[png:simple-planning/5]]

** The current most successful heuristic function : Delete Relaxation

 [[png:simple-planning/relax7]]

** The current most successful heuristic function : Delete Relaxation

 [[png:simple-planning/relax6]]

** The current most successful heuristic function : Delete Relaxation

 [[png:simple-planning/relax4]]

** The current most successful heuristic function : Delete Relaxation

 [[png:simple-planning/relax3]]

** The current most successful heuristic function : Delete Relaxation

 [[png:simple-planning/relax2]]

** The current most successful heuristic function : Delete Relaxation

 [[png:simple-planning/relax1]]

** The current most successful heuristic function : Delete Relaxation

 [[png:simple-planning/relax]]

* Latplan

[[png:dsama/planning4]]

** AMA_2 is black box → Primitive GoalCount heuristics

[[png:dsama/planning5]]

** AMA_2 AAE/AD cheated with a custom planner                      :noexport:

[[png:dsama/planning6]]

** SotA Planners need */Descriptive Action Models/* : PDDL

[[png:dsama/planning7]]

** Heuristics need */Descriptive Action Models/* : PDDL            :noexport:

[[png:dsama/planning8]]

#+begin_quote
#+begin_center
To */unleash the full potential/ of off-the-shelf planners*

*/Descriptive action model/ is the next milestone*
#+end_center
#+end_quote

** Double-Stage AMA (Arxiv 1912.05492)

[[png:dsama/dsama1]]

** Double-Stage AMA (Arxiv 1912.05492)

[[png:dsama/dsama2]]

** Double-Stage AMA (Arxiv 1912.05492)

[[png:dsama/dsama3]]

#+begin_center
*Failed:* Domain file *too large* & *too many disjunctions*
#+end_center

** Good Accuracy                                                   :noexport:

F-measure for effects

[[spng:dsama/AAE]]

** Good Accuracy                                                   :noexport:

F-measure for preconditions

[[spng:dsama/AD]]

** Double-Stage AMA (Arxiv 1912.05492)                             :noexport:

[[png:dsama/dsama4]]

** Double-Stage AMA (Arxiv 1912.05492)                             :noexport:

[[png:dsama/dsama5]]

** Fail: Ridiculously Large PDDL                                   :noexport:

[[spng:dsama/size]]

Vertical axis: Number of trees $T$ in Random Forest

Horizontal axis: Tree depth $D$

** Fast Downward fails @@html:<br>@@ /during the input processing/ :noexport:


Reasons?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
+ Large file size
+ Heavy usage of disjunctions =(or ...)=
  + *must be removed*, called /flattening/
#+end_span8
#+begin_span4
[[spng:dsama/size]]
#+end_span4
#+end_row-fluid
#+end_container-fluid

#+begin_center
+ _Flattening = Compiling a Decision Tree into_
  
  _an exponential number of Decision Lists_

#+begin_larger
+ _Action models learned by NNs are too complex!_
#+end_larger
#+end_center


* AAE's progression is a */black box/*

#+begin_quote
*/AAE's progression:/* 　 $z_{t+1} = \text{apply}(z_t, a)$

_/STRIPS progression:/_ $z_{t+1} = (z_t \ \setminus \ \text{del}(a)\ )\ \cap\ \text{add}(a)$
#+end_quote

+ *Issue1*: *Effects must be disentangled from $z_t$*.

  #+begin_alignright
  (\rightarrow read the paper, _/STRIPS = Cube-Like Graph from graph-theory./_)
  #+end_alignright

  　

+ *Issue2*: $Z$ *is fixed* $\longleftarrow$ *$Z$ and $A$ are trained separately*

  #+begin_alignright
  $Z$ may not have compact STRIPS models
  #+end_alignright

  　


#+begin_larger
#+begin_center
+ *Solution*: _Train $Z$ and $A$ end-to-end_ + _Restrict $A$ to STRIPS_

  $\rightarrow$ *$Z$ must adapt to $A$ (STRIPS)*
#+end_center
#+end_larger

* End-to-End NN: Cube-Space AE

[[png:space-ae/1-sae]]

** End-to-End NN: Cube-Space AE

[[png:space-ae/2-1]]

** End-to-End NN: Cube-Space AE

[[png:space-ae/2-2]]

** End-to-End NN: Cube-Space AE

[[png:space-ae/2-3]]

** End-to-End NN: Cube-Space AE                                    :noexport:

[[png:space-ae/2-4]]
** End-to-End NN: Cube-Space AE                                    :noexport:

[[png:space-ae/2-3h]]

** End-to-End NN: Cube-Space AE

[[png:space-ae/2-5]]

* Fixing the AAE: Disentangling effects from states

[[png:space-ae/3-1]]

** Min = delete effect / set intersection @@html:<br>@@ Max = add effect / set union

This turned out to be hard to train

[[png:space-ae/3-2]]


** Fixing the AAE: Disentangling effects from states               :noexport:

[[png:space-ae/3-3]]

** Fixing the AAE: Disentangling effects from states               :noexport:

[[png:space-ae/3-3h]]

** Proposed: */Back-to-Logit/* = continuous effects

+ (1) *Make propositional (0/1) states continuous (/back to logit!/)*

  #+begin_alignright
  🚗🔥🔥🔥🔥🔥🕙🏛🗲🗲
  #+end_alignright

  　 ← by a monotonic conversion : Batch Normalization

+ (2) *perform a numeric addition*,
+ (3) *discretize the result → propositional successor state*

[[png:space-ae/3-5]]

** Results: Better training (ablation study)

[[png:results/ablation]]

+ (2) BTL is essential; min/max does not work
+ (3) Batchnorm is essential; Other losses are also essential

* To PDDL: Extracting action effects (details ommited)

Dataset $\{ (z_t, z_{t+1}) \mid \text{action}(z_t, z_{t+1})=a\}$:

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
If $\exists t; (z_{t,f},z_{t+1,f})=(0,1)$ then $f\in\text{add}(a)$.

If $\exists t; (z_{t,f},z_{t+1,f})=(1,0)$ then $f\in\text{del}(a)$.

#+end_span8
#+begin_span4
[[png:space-ae/3-5]]
#+end_span4
#+end_row-fluid
#+end_container-fluid

*Theorem*: */BtL guarantees that add and delete-effect never coexist./*

#+begin_quote
Proof sketch: Both BatchNorm and Discrete are monotonic, and we perform arithmetic additions.
When $(z_{t,f},z_{t+1,f})=(0,1)$ for some $t$, thus the numeric effect is positive, therefore
$(1,1)$ may happen but $(1,0)$ cannot happen. $(0,0)$ does not happen because the numeric effect is a constant for each $a,f$.
#+end_quote

* To PDDL: Extracting action preconditions

Dataset $\{ (z_t, z_{t+1}) \mid \text{action}(z_t, z_{t+1})=a\}$:

Extracts a *single* decision rule (not decision list or trees)

containing _the bit value shared by all data points_ of this action $a$:

\[
 \text{pre}(a) = \{f|\forall t;z_{t,f}=1\}\cup \{\lnot f|\forall t; z_{t,f}=0\}
\] 

$f$: index in the binary vector

#+begin_alignright
(similar to Amado et al, 2018)
#+end_alignright

* Example PDDL output from our system

[[spng:results/fig1]]

* Results : Ablation study                                         :noexport:

[[spng:results/table1]]

* Results : Blind search eats up 2GB memory and dies. @@html:<br>@@ SotA heuristics (LMcut/M&S) scale up the search!


#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
SAE+AAE trained separately
#+end_span6
#+begin_span6
_*Cube-Space AE*_
#+end_span6
#+end_row-fluid
#+end_container-fluid


[[spng:results/table2]]

#+begin_center
_*First demonstration* of the meaningful speedup_

_by the domain-independent heuristics_

in the latent space.
#+end_center

* Results : Confirms the search efficiency

[[spng:results/fig2]]

#+begin_quote
*Significance*:
Unlike (the hyped) RL approach,
it is _never trained on the domain-dependent, reward-shaped information._
It also does not require crazy amount of training.
#+end_quote

* Results : Strong Generalization

#+begin_center
Number of states $S$ & transitions $T$

just "copied" from the training dataset
#+end_center

　

#+begin_container-fluid

#+begin_container-row
#+begin_span2

#+end_span2
#+begin_span8
[[spng:results/generalization]]
#+end_span8
#+begin_span2

#+end_span2
#+end_container-row
#+end_container-fluid

#+begin_center
Mostly $S=T=0$

　

*Produced plans did not contain training examples!!*
#+end_center

* Conclusion : New Milestone Achieved

+ */Cube-Space AE/* : Learns STRIPS planning models from images
  + *SotA heuristic search now available in learned representations*
    + The central focus of planning community for >20 years
+ Unlike RL, */classical planning heuristics/* requires
  + *No policy learning* from the environment
  + *Domain-independent*; Works on first-seen environment
  + *Guarantees optimality*

#+begin_larger
#+begin_center
+ */New Era:/ Symbolic approach will kick the butt of RL*

  *with 0 policy training!*
#+end_center
#+end_larger

